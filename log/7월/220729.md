## 220729

<br>

## 오전 7시 - 9시

<br>

<이상 찾기>


1. 예전엔 scheduler 없이도 잘 됐는데, 지금은 없으면 lr 이 커서 터진다.

2. 예전 성능의 최대치에 도달하지 못한다.

----

[지금 돌리는 실험]: no scheduler, lr=1e-5

- valid acc drastically drops
    + lr 문제: 우리는 lr 문제는 이미 해결함.
    + pytorch 버전 문제: 0.4 버전에서 생기는 옛날 문제다. 현재는 1.7.1 이다.

- batch 에서 tuple 이 아닌 dict 방식으로 넘기는 게 문제인가?
    + 구글링해봤지만, 관련 내용보다 dict to tensor 와 같이 평범한 글들만 나온다.

- reshape, view 의 문제인가?
    + 예전 코드에도 reshape 를 씀.

- trainer 비교: 문제 없음.

- dtype 실험
    + collate_fn, trainer, encoder_decoder_model 로 전달되면서 생기는 문제는 없었음.


결론: 원인 파악을 1주일 정도 소요하며 진행했으나, 파악할 수 없어 예전 상태로 되돌아가기로 함.

<br>

# 오전 10시 - 오후 12시

<br>

1. old 에서 new 추가된 기능 추가

- evaluator ACCURACY 수정
- Logger.py 추가
- EarlyStop.py 추가
- Logger py file 저장 기능 추가
- new AGC dataset 추가

<br>

# 오후 1시 - 6시, 오전 12시 - 2시

2. Transformer Decoder -> 등장한 토큰만 적용
    - 적용하니, 학습 자체를 못함.
    - 학습자체에 문제가 생기는 것 같음.

    